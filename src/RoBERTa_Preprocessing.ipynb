{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RoBERTa_Preprocessing",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EIdasRsYzRo0"
      },
      "source": [
        "# Mount drive\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nAo5FR2EzQ0l",
        "outputId": "10734a17-eb01-4c69-eb2a-9103edfcf687"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "# change this to your desired directory\n",
        "ROOT = '/content/drive/MyDrive/Machine_Learning'\n",
        "# and this too\n",
        "ROOT_DATA = f'{ROOT}/shopee_sentiment_data_set'\n",
        "\n",
        "!ls $ROOT"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "cache  colab\t\t   envibert_original\n",
            "ckpt   envibert_augmented  shopee_sentiment_data_set\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r01jdkFPz1bw"
      },
      "source": [
        "# Install packages\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Y1RRrmLzx3g",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "3e1094d8-9b1e-43c5-da55-f0e2f3e755e7"
      },
      "source": [
        "!pip install fairseq\n",
        "!pip install pytorch-lightning\n",
        "!pip install transformers\n",
        "!pip install sentencepiece"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fairseq\n",
            "  Downloading fairseq-0.10.2-cp37-cp37m-manylinux1_x86_64.whl (1.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.7 MB 5.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from fairseq) (2019.12.20)\n",
            "Collecting dataclasses\n",
            "  Downloading dataclasses-0.6-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: cffi in /usr/local/lib/python3.7/dist-packages (from fairseq) (1.15.0)\n",
            "Collecting sacrebleu>=1.4.12\n",
            "  Downloading sacrebleu-2.0.0-py3-none-any.whl (90 kB)\n",
            "\u001b[K     |████████████████████████████████| 90 kB 8.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from fairseq) (1.19.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from fairseq) (4.62.3)\n",
            "Collecting hydra-core\n",
            "  Downloading hydra_core-1.1.1-py3-none-any.whl (145 kB)\n",
            "\u001b[K     |████████████████████████████████| 145 kB 62.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: cython in /usr/local/lib/python3.7/dist-packages (from fairseq) (0.29.24)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from fairseq) (1.10.0+cu111)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.7/dist-packages (from sacrebleu>=1.4.12->fairseq) (0.8.9)\n",
            "Collecting colorama\n",
            "  Downloading colorama-0.4.4-py2.py3-none-any.whl (16 kB)\n",
            "Collecting portalocker\n",
            "  Downloading portalocker-2.3.2-py2.py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi->fairseq) (2.21)\n",
            "Collecting omegaconf==2.1.*\n",
            "  Downloading omegaconf-2.1.1-py3-none-any.whl (74 kB)\n",
            "\u001b[K     |████████████████████████████████| 74 kB 3.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from hydra-core->fairseq) (5.4.0)\n",
            "Collecting antlr4-python3-runtime==4.8\n",
            "  Downloading antlr4-python3-runtime-4.8.tar.gz (112 kB)\n",
            "\u001b[K     |████████████████████████████████| 112 kB 45.4 MB/s \n",
            "\u001b[?25hCollecting PyYAML>=5.1.0\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 57.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from importlib-resources->hydra-core->fairseq) (3.6.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->fairseq) (3.10.0.2)\n",
            "Building wheels for collected packages: antlr4-python3-runtime\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.8-py3-none-any.whl size=141230 sha256=7132fdc855f172d71930466bfbf64f8c83a8fddcc3cd5e1d7cc760348b691dbb\n",
            "  Stored in directory: /root/.cache/pip/wheels/ca/33/b7/336836125fc9bb4ceaa4376d8abca10ca8bc84ddc824baea6c\n",
            "Successfully built antlr4-python3-runtime\n",
            "Installing collected packages: PyYAML, antlr4-python3-runtime, portalocker, omegaconf, colorama, sacrebleu, hydra-core, dataclasses, fairseq\n",
            "  Attempting uninstall: PyYAML\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed PyYAML-6.0 antlr4-python3-runtime-4.8 colorama-0.4.4 dataclasses-0.6 fairseq-0.10.2 hydra-core-1.1.1 omegaconf-2.1.1 portalocker-2.3.2 sacrebleu-2.0.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "pydevd_plugins"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pytorch-lightning\n",
            "  Downloading pytorch_lightning-1.5.3-py3-none-any.whl (523 kB)\n",
            "\u001b[K     |████████████████████████████████| 523 kB 5.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorboard>=2.2.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning) (2.7.0)\n",
            "Collecting pyDeprecate==0.3.1\n",
            "  Downloading pyDeprecate-0.3.1-py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning) (6.0)\n",
            "Collecting fsspec[http]!=2021.06.0,>=2021.05.0\n",
            "  Downloading fsspec-2021.11.1-py3-none-any.whl (132 kB)\n",
            "\u001b[K     |████████████████████████████████| 132 kB 60.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning) (3.10.0.2)\n",
            "Requirement already satisfied: torch>=1.6 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning) (1.10.0+cu111)\n",
            "Collecting future>=0.17.1\n",
            "  Downloading future-0.18.2.tar.gz (829 kB)\n",
            "\u001b[K     |████████████████████████████████| 829 kB 41.5 MB/s \n",
            "\u001b[?25hCollecting torchmetrics>=0.4.1\n",
            "  Downloading torchmetrics-0.6.0-py3-none-any.whl (329 kB)\n",
            "\u001b[K     |████████████████████████████████| 329 kB 53.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning) (21.3)\n",
            "Requirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning) (4.62.3)\n",
            "Requirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning) (1.19.5)\n",
            "Collecting aiohttp\n",
            "  Downloading aiohttp-3.8.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 41.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (2.23.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=17.0->pytorch-lightning) (3.0.6)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (1.42.0)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (0.12.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (3.3.6)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (0.4.6)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (3.17.3)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (57.4.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (1.35.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (1.8.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (0.37.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (0.6.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from absl-py>=0.4->tensorboard>=2.2.0->pytorch-lightning) (1.15.0)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning) (4.2.4)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning) (4.7.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard>=2.2.0->pytorch-lightning) (4.8.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard>=2.2.0->pytorch-lightning) (3.6.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning) (0.4.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (1.24.3)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning) (3.1.1)\n",
            "Collecting frozenlist>=1.1.1\n",
            "  Downloading frozenlist-1.2.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (192 kB)\n",
            "\u001b[K     |████████████████████████████████| 192 kB 54.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (2.0.7)\n",
            "Collecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-5.2.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (160 kB)\n",
            "\u001b[K     |████████████████████████████████| 160 kB 54.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (21.2.0)\n",
            "Collecting yarl<2.0,>=1.0\n",
            "  Downloading yarl-1.7.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (271 kB)\n",
            "\u001b[K     |████████████████████████████████| 271 kB 76.2 MB/s \n",
            "\u001b[?25hCollecting aiosignal>=1.1.2\n",
            "  Downloading aiosignal-1.2.0-py3-none-any.whl (8.2 kB)\n",
            "Collecting async-timeout<5.0,>=4.0.0a3\n",
            "  Downloading async_timeout-4.0.1-py3-none-any.whl (5.7 kB)\n",
            "Collecting asynctest==0.13.0\n",
            "  Downloading asynctest-0.13.0-py3-none-any.whl (26 kB)\n",
            "Building wheels for collected packages: future\n",
            "  Building wheel for future (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for future: filename=future-0.18.2-py3-none-any.whl size=491070 sha256=3c23f6693c3627e5e9908d1f95c00996ec74686e23bdde72827cb2720033edc4\n",
            "  Stored in directory: /root/.cache/pip/wheels/56/b0/fe/4410d17b32f1f0c3cf54cdfb2bc04d7b4b8f4ae377e2229ba0\n",
            "Successfully built future\n",
            "Installing collected packages: multidict, frozenlist, yarl, asynctest, async-timeout, aiosignal, fsspec, aiohttp, torchmetrics, pyDeprecate, future, pytorch-lightning\n",
            "  Attempting uninstall: future\n",
            "    Found existing installation: future 0.16.0\n",
            "    Uninstalling future-0.16.0:\n",
            "      Successfully uninstalled future-0.16.0\n",
            "Successfully installed aiohttp-3.8.1 aiosignal-1.2.0 async-timeout-4.0.1 asynctest-0.13.0 frozenlist-1.2.0 fsspec-2021.11.1 future-0.18.2 multidict-5.2.0 pyDeprecate-0.3.1 pytorch-lightning-1.5.3 torchmetrics-0.6.0 yarl-1.7.2\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.12.5-py3-none-any.whl (3.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.1 MB 5.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.4.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Collecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.1.2-py3-none-any.whl (59 kB)\n",
            "\u001b[K     |████████████████████████████████| 59 kB 5.7 MB/s \n",
            "\u001b[?25hCollecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 58.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.8.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.46-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 58.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.6)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.6.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Installing collected packages: tokenizers, sacremoses, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.1.2 sacremoses-0.0.46 tokenizers-0.10.3 transformers-4.12.5\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 5.0 MB/s \n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.96\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XvM4re5Ky_j5"
      },
      "source": [
        "# Pre-processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hAXfGasUy14g",
        "outputId": "a5d1fdd2-19d9-4d3a-b5bb-f6fc4701079c"
      },
      "source": [
        "import json\n",
        "import math\n",
        "import pandas as pd\n",
        "import re\n",
        "import requests\n",
        "import unicodedata\n",
        "\n",
        "# preview files\n",
        "def preview_file():\n",
        "    global ROOT_DATA\n",
        "    data = pd.read_csv(f'{ROOT_DATA}/train_preprocess_unsegment.csv')\n",
        "    print(data['text'].tail)\n",
        "    print(data['preprocess_text'].tail)\n",
        "    data = pd.read_csv(f'{ROOT_DATA}/p_train.csv')\n",
        "    print(data['text'].tail)\n",
        "\n",
        "# removing spaces adjacent to accents\n",
        "def rm_accents_spaces(s: str):\n",
        "    # the accents looks pretty cool right lol\n",
        "    accents = ['̀', '̃', '́', '̉', '̣']\n",
        "\n",
        "    i = 1\n",
        "    while i < len(s):\n",
        "        if s[i] in accents:\n",
        "            s = f'{s[:i-1]}{s[i]}{s[i+2:]}'\n",
        "        i += 1\n",
        "\n",
        "    return s\n",
        "\n",
        "# segment words that are stucked together using word frequency\n",
        "def segment_word(word: str, word_dict: dict):\n",
        "    result = []\n",
        "    space = []\n",
        "    cost = [0]\n",
        "    for i in range(1, len(word)+1):\n",
        "        min_cost = float('inf')\n",
        "        p = 0\n",
        "\n",
        "        for j in range(1, i+1):\n",
        "            w = word[max(0, i-j):i]\n",
        "            w = word_dict.get(w, {'appearance': 0, 'cost': float('inf')})\n",
        "            c = cost[max(0, i-j)] + w['cost']\n",
        "            if min_cost > c:\n",
        "                min_cost = c\n",
        "                p = max(0, i-j)\n",
        "\n",
        "        space.append(p)\n",
        "        cost.append(min_cost)\n",
        "\n",
        "    p = len(space)-1\n",
        "    while(p >= 0):\n",
        "        result.append(word[space[p]:p+1])\n",
        "        p = space[p]-1\n",
        "\n",
        "    result.reverse()\n",
        "\n",
        "    return result\n",
        "\n",
        "# segment a sentence\n",
        "def segment_sentence(sentence: str, word_dict: dict):\n",
        "    # spliting the sentence\n",
        "    words = sentence.split(' ')\n",
        "    words = list(filter(None, words))\n",
        "    result = []\n",
        "    # segment each word\n",
        "    for word in words:\n",
        "        result.extend(segment_word(word, word_dict))\n",
        "    result = ' '.join(result)\n",
        "    return result\n",
        "\n",
        "# load word dictionary\n",
        "def load_word_dict(cached: bool = False):\n",
        "    # Check if cached\n",
        "    if cached:\n",
        "        with open(f'{ROOT_DATA}/word_dict.json', 'r', encoding='utf8') as f:\n",
        "            word_dict = json.load(f)\n",
        "    else:\n",
        "        # Get the online data\n",
        "        print('Fetching word list . . .')\n",
        "        url = 'https://raw.githubusercontent.com/garfieldnate/vi_experiments/master/wiki_word_list/wikipedia_unigrams.txt'\n",
        "        response = requests.get(url)\n",
        "\n",
        "        # Parse the data\n",
        "        print('Processing word list . . .')\n",
        "        raw = response.text\n",
        "        lines = raw.split('\\n')\n",
        "        lines = lines[1:]\n",
        "        word_dict = {}\n",
        "        total_count = 0\n",
        "        for line in lines:\n",
        "            tmp = line.split('\\t')\n",
        "            if len(tmp) == 2:\n",
        "                appearance = int(tmp[1])\n",
        "                total_count += appearance\n",
        "                word_dict[re.sub(r'\\s+', '_', tmp[0])] = {\n",
        "                    'appearance': appearance,\n",
        "                    'cost': 0\n",
        "                }\n",
        "\n",
        "        for i in range(101):\n",
        "            word_dict[str(i)] = {\n",
        "                'appearance': 100,\n",
        "                'cost': 0\n",
        "            }\n",
        "            total_count += 100\n",
        "\n",
        "        for word in word_dict:\n",
        "            word_dict[word]['cost'] = math.log(\n",
        "                total_count/word_dict[word]['appearance']\n",
        "            )\n",
        "\n",
        "        # Cache the data\n",
        "        print('Caching word list . . .')\n",
        "        with open(f'{ROOT_DATA}/word_dict.json', 'w+', encoding='utf8') as f:\n",
        "            json.dump(word_dict, f)\n",
        "\n",
        "    return word_dict\n",
        "\n",
        "# preprocess starts here\n",
        "def preprocess(filename: str = 'train.csv', field: str = 'text', underscore_mode: bool = False):\n",
        "    global ROOT_DATA\n",
        "    data = pd.read_csv(f'{ROOT_DATA}/{filename}')\n",
        "\n",
        "    reviews = data[field]\n",
        "    p_reviews = []\n",
        "\n",
        "    count = 0\n",
        "    word_list = load_word_dict()\n",
        "\n",
        "    for review in reviews:\n",
        "        count += 1\n",
        "        if count % 1000 == 1:\n",
        "            print(f'Processing row {count}-{min(len(reviews), count+999)}')\n",
        "\n",
        "        review = review.lower()\n",
        "        review = re.sub(r'\\s+', ' ', review)\n",
        "        review = rm_accents_spaces(review)\n",
        "        review = unicodedata.normalize('NFC', review)\n",
        "        review = re.sub(r'\\s*\\W\\s*', ' ', review)\n",
        "        review = re.sub(r'\\s+', ' ', review)\n",
        "        review = segment_sentence(review, word_list)\n",
        "        if underscore_mode is False:\n",
        "            review = re.sub(r'_', ' ', review)\n",
        "        review = re.sub(r'\\s+', ' ', review)\n",
        "        p_reviews.append(review)\n",
        "\n",
        "    data[field] = p_reviews\n",
        "    data.to_csv(f'{ROOT_DATA}/p_{filename}')\n",
        "\n",
        "# main\n",
        "if __name__ == '__main__':\n",
        "    preprocess('train.csv', 'text', False)\n",
        "    preprocess('test.csv', 'text', False)\n",
        "    preview_file()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetching word list . . .\n",
            "Processing word list . . .\n",
            "Caching word list . . .\n",
            "Processing row 1-1000\n",
            "Processing row 1001-2000\n",
            "Processing row 2001-3000\n",
            "Processing row 3001-4000\n",
            "Processing row 4001-5000\n",
            "Processing row 5001-6000\n",
            "Processing row 6001-7000\n",
            "Processing row 7001-8000\n",
            "Processing row 8001-9000\n",
            "Processing row 9001-10000\n",
            "Processing row 10001-11000\n",
            "Processing row 11001-12000\n",
            "Processing row 12001-13000\n",
            "Processing row 13001-14000\n",
            "Processing row 14001-15000\n",
            "Processing row 15001-16000\n",
            "Processing row 16001-17000\n",
            "Processing row 17001-18000\n",
            "Processing row 18001-19000\n",
            "Processing row 19001-20000\n",
            "Processing row 20001-21000\n",
            "Processing row 21001-22000\n",
            "Processing row 22001-23000\n",
            "Processing row 23001-24000\n",
            "Processing row 24001-25000\n",
            "Processing row 25001-26000\n",
            "Processing row 26001-27000\n",
            "Fetching word list . . .\n",
            "Processing word list . . .\n",
            "Caching word list . . .\n",
            "Processing row 1-1000\n",
            "Processing row 1001-2000\n",
            "Processing row 2001-3000\n",
            "<bound method NDFrame.tail of 0        Đến quán 2 lần thôi , rất là thích !\\nQuán tuy...\n",
            "1        Đến quán vào tối chủ_nhật . Có band hát . Khá ...\n",
            "2        Phục_vụ lâu quá mặc_dù khách rất vắng .\\nĐợi g...\n",
            "3        Ko gian bé_tí , quán chật_chội , đông người nê...\n",
            "4        Khi mình order , đặt bánh thì nhận được sự tiế...\n",
            "                               ...                        \n",
            "26995    Không_gian đẹp . Đồ uống bình thg . Cheese ngo...\n",
            "26996    Chỉ có hai từ thất_vọng mới diễn_tả được cảm_g...\n",
            "26997    Hôm vào quán có bói bài Tarot , nên cũng bon_c...\n",
            "26998    Va ̀ o ngô ̀ i đơ ̣ i 20p mơ ́ i đươ ̣ c phu ̣...\n",
            "26999    Chổ hình_như thấy que quen , hình_như là ở trê...\n",
            "Name: text, Length: 27000, dtype: object>\n",
            "<bound method NDFrame.tail of 0        đến quán 2 lần thôi rất là thích quán tuy nằm ...\n",
            "1        đến quán vào tối chủ nhật có band hát khá ổn t...\n",
            "2        phục vụ lâu quá mặc dù khách rất vắng đợi gần ...\n",
            "3        ko gian bé tí   quán chật chội đông người nên ...\n",
            "4        khi mình order đặt bánh thì nhận được sự tiếp ...\n",
            "                               ...                        \n",
            "26995    không gian đẹp đồ uống bình thg chese ngon tan...\n",
            "26996    chỉ có hai từ thất vọng mới diễn tả được cảm g...\n",
            "26997    hôm vào quán có bói bài tarot nên cũng bon che...\n",
            "26998    vào ngồi đợi 20 p mới được phục vụ 2 cốc freze...\n",
            "26999    chổ hình như thấy que quen hình như là ở trên ...\n",
            "Name: preprocess_text, Length: 27000, dtype: object>\n",
            "<bound method NDFrame.tail of 0        đến quán 2 lần thôi rất là thích quán tuy nằm ...\n",
            "1        đến quán vào tối chủ nhật có band hát khá ổn t...\n",
            "2        phục vụ lâu quá mặc dù khách rất vắng đợi gần ...\n",
            "3        ko gian bé tí quán chật chội đông người nên ph...\n",
            "4        khi mình order đặt bánh thì nhận được sự tiếp ...\n",
            "                               ...                        \n",
            "26995    không gian đẹp đồ uống bình thg cheese ngon ta...\n",
            "26996    chỉ có hai từ thất vọng mới diễn tả được cảm g...\n",
            "26997    hôm vào quán có bói bài tarot nên cũng bon che...\n",
            "26998    vào ngồi đợi 20 p mới được phục vụ 2 cốc freez...\n",
            "26999    chổ hình như thấy que quen hình như là ở trên ...\n",
            "Name: text, Length: 27000, dtype: object>\n"
          ]
        }
      ]
    }
  ]
}